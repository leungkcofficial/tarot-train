"""
Verify the sanity of CIF arrays generated by the ensemble models.

This script performs comprehensive checks on the Cumulative Incidence Function (CIF)
arrays to ensure they are valid and reasonable.
"""

import os
import json
import h5py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Tuple, Any
from pathlib import Path


class NumpyEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy types."""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        return super(NumpyEncoder, self).default(obj)


class CIFVerifier:
    """Class to verify CIF array sanity."""
    
    def __init__(self, predictions_dir: str = "results/final_deploy/individual_predictions"):
        self.predictions_dir = predictions_dir
        self.summary_path = os.path.join(predictions_dir, "prediction_generation_summary_20250808_113457.json")
        self.results = {
            "checks": {},
            "statistics": {},
            "warnings": [],
            "errors": []
        }
        
    def load_summary(self) -> Dict:
        """Load the prediction generation summary."""
        with open(self.summary_path, 'r') as f:
            return json.load(f)
    
    def load_predictions(self, h5_path: str) -> np.ndarray:
        """Load predictions from H5 file."""
        with h5py.File(h5_path, 'r') as f:
            return f['predictions'][:]
    
    def check_value_range(self, cif: np.ndarray, model_name: str) -> bool:
        """Check if all CIF values are between 0 and 1."""
        min_val = np.min(cif)
        max_val = np.max(cif)
        
        passed = (min_val >= 0) and (max_val <= 1)
        
        self.results["checks"][f"{model_name}_value_range"] = {
            "passed": passed,
            "min_value": float(min_val),
            "max_value": float(max_val)
        }
        
        if not passed:
            self.results["errors"].append(f"{model_name}: Values outside [0,1] range")
        
        return passed
    
    def check_monotonicity(self, cif: np.ndarray, model_name: str, model_type: str) -> bool:
        """Check if CIF values are non-decreasing over time."""
        if model_type == "deepsurv":
            # For DeepSurv: check along time axis (axis=0)
            diffs = np.diff(cif, axis=0)
            time_axis = 0
        else:  # deephit
            # For DeepHit: check along time axis (axis=1)
            diffs = np.diff(cif, axis=1)
            time_axis = 1
        
        # Allow small numerical errors
        passed = np.all(diffs >= -1e-6)
        
        if not passed:
            n_violations = np.sum(diffs < -1e-6)
            self.results["warnings"].append(
                f"{model_name}: {n_violations} monotonicity violations found"
            )
        
        self.results["checks"][f"{model_name}_monotonicity"] = {
            "passed": passed,
            "violations": int(np.sum(diffs < -1e-6)) if not passed else 0
        }
        
        return passed
    
    def check_competition(self, cif: np.ndarray, model_name: str) -> bool:
        """Check if sum of CIFs for both events doesn't exceed 1 (DeepHit only)."""
        # Sum event 1 and event 2
        cif_sum = cif[0] + cif[1]
        
        # Allow small numerical errors
        passed = np.all(cif_sum <= 1.0 + 1e-6)
        
        max_sum = np.max(cif_sum)
        
        self.results["checks"][f"{model_name}_competition"] = {
            "passed": passed,
            "max_sum": float(max_sum)
        }
        
        if not passed:
            self.results["errors"].append(
                f"{model_name}: Competition constraint violated (max sum: {max_sum:.4f})"
            )
        
        return passed
    
    def check_initial_values(self, cif: np.ndarray, model_name: str, model_type: str) -> bool:
        """Check if CIF starts near 0 at time 0."""
        if model_type == "deepsurv":
            initial_mean = np.mean(cif[0])
        else:  # deephit
            initial_mean = np.mean(cif[:, 0, :])
        
        # Should be close to 0
        passed = initial_mean < 0.05  # Allow up to 5% at start
        
        self.results["checks"][f"{model_name}_initial_values"] = {
            "passed": passed,
            "initial_mean": float(initial_mean)
        }
        
        if not passed:
            self.results["warnings"].append(
                f"{model_name}: High initial CIF values (mean: {initial_mean:.4f})"
            )
        
        return passed
    
    def check_final_values(self, cif: np.ndarray, model_name: str, model_type: str, event_type: str) -> bool:
        """Check if final CIF values are in reasonable range."""
        if model_type == "deepsurv":
            final_mean = np.mean(cif[-1])
        else:  # deephit
            # Check both events
            final_mean_event1 = np.mean(cif[0, -1, :])
            final_mean_event2 = np.mean(cif[1, -1, :])
            final_mean = (final_mean_event1, final_mean_event2)
        
        # Expected ranges (adjust based on your domain)
        if model_type == "deepsurv":
            if "Event_1" in model_name:
                expected_range = (0.05, 0.30)  # 5-30% for Event 1
            else:
                expected_range = (0.10, 0.40)  # 10-40% for Event 2
            
            passed = expected_range[0] <= final_mean <= expected_range[1]
            
            self.results["checks"][f"{model_name}_final_values"] = {
                "passed": passed,
                "final_mean": float(final_mean),
                "expected_range": expected_range
            }
        else:
            # For DeepHit, check both events
            passed = True
            self.results["checks"][f"{model_name}_final_values"] = {
                "passed": passed,
                "final_mean_event1": float(final_mean[0]),
                "final_mean_event2": float(final_mean[1])
            }
        
        return passed
    
    def check_nan_inf(self, cif: np.ndarray, model_name: str) -> bool:
        """Check for NaN or Inf values."""
        has_nan = np.any(np.isnan(cif))
        has_inf = np.any(np.isinf(cif))
        
        passed = not (has_nan or has_inf)
        
        self.results["checks"][f"{model_name}_nan_inf"] = {
            "passed": passed,
            "has_nan": bool(has_nan),
            "has_inf": bool(has_inf)
        }
        
        if not passed:
            self.results["errors"].append(f"{model_name}: Contains NaN or Inf values")
        
        return passed
    
    def check_shape(self, cif: np.ndarray, model_name: str, expected_shape: Tuple) -> bool:
        """Check if shape matches expected dimensions."""
        actual_shape = cif.shape
        passed = actual_shape == expected_shape
        
        self.results["checks"][f"{model_name}_shape"] = {
            "passed": passed,
            "actual_shape": actual_shape,
            "expected_shape": expected_shape
        }
        
        if not passed:
            self.results["errors"].append(
                f"{model_name}: Shape mismatch. Expected {expected_shape}, got {actual_shape}"
            )
        
        return passed
    
    def compute_statistics(self, cif: np.ndarray, model_name: str, model_type: str):
        """Compute summary statistics for the CIF array."""
        stats = {
            "min": float(np.min(cif)),
            "max": float(np.max(cif)),
            "mean": float(np.mean(cif)),
            "std": float(np.std(cif))
        }
        
        if model_type == "deepsurv":
            # Add time-specific stats
            stats["mean_at_1year"] = float(np.mean(cif[364]))  # Day 365
            stats["mean_at_3years"] = float(np.mean(cif[1094]))  # Day 1095
            stats["mean_at_5years"] = float(np.mean(cif[-1]))
        else:  # deephit
            # Add event-specific stats
            stats["event1_mean_at_5years"] = float(np.mean(cif[0, -1, :]))
            stats["event2_mean_at_5years"] = float(np.mean(cif[1, -1, :]))
        
        self.results["statistics"][model_name] = stats
    
    def verify_model(self, model_info: Dict, dataset: str = "temporal") -> Dict[str, bool]:
        """Run all checks for a single model."""
        model_name = model_info["model_name"]
        model_type = model_info["model_type"]
        model_number = model_info["model_number"]
        
        # Load predictions
        if dataset == "temporal":
            h5_path = model_info["temporal_predictions_path"]
            expected_n_samples = model_info["temporal_n_samples"]
        else:
            h5_path = model_info["spatial_predictions_path"]
            expected_n_samples = model_info["spatial_n_samples"]
        
        print(f"\nVerifying {model_name} ({dataset})...")
        
        try:
            cif = self.load_predictions(h5_path)
            
            # Determine expected shape
            if model_type == "deepsurv":
                expected_shape = (1825, expected_n_samples)
            else:  # deephit
                expected_shape = (2, 5, expected_n_samples)
            
            # Run checks
            checks = {
                "shape": self.check_shape(cif, f"{model_name}_{dataset}", expected_shape),
                "nan_inf": self.check_nan_inf(cif, f"{model_name}_{dataset}"),
                "value_range": self.check_value_range(cif, f"{model_name}_{dataset}"),
                "monotonicity": self.check_monotonicity(cif, f"{model_name}_{dataset}", model_type),
                "initial_values": self.check_initial_values(cif, f"{model_name}_{dataset}", model_type),
                "final_values": self.check_final_values(cif, f"{model_name}_{dataset}", model_type, model_name)
            }
            
            # Additional check for DeepHit
            if model_type == "deephit":
                checks["competition"] = self.check_competition(cif, f"{model_name}_{dataset}")
            
            # Compute statistics
            self.compute_statistics(cif, f"{model_name}_{dataset}", model_type)
            
            # Summary
            all_passed = all(checks.values())
            print(f"  ✓ All checks passed" if all_passed else f"  ✗ Some checks failed")
            
            return checks
            
        except Exception as e:
            self.results["errors"].append(f"{model_name}_{dataset}: Error loading/processing - {str(e)}")
            print(f"  ✗ Error: {str(e)}")
            return {}
    
    def create_visualizations(self, output_dir: str = "results/final_deploy/verification"):
        """Create visualization plots for verification results."""
        os.makedirs(output_dir, exist_ok=True)
        
        # Plot 1: Summary of check results
        fig, ax = plt.subplots(figsize=(12, 8))
        
        check_types = ["shape", "nan_inf", "value_range", "monotonicity", "initial_values", "final_values", "competition"]
        check_counts = {check: {"passed": 0, "failed": 0} for check in check_types}
        
        for key, value in self.results["checks"].items():
            for check_type in check_types:
                if check_type in key:
                    if value["passed"]:
                        check_counts[check_type]["passed"] += 1
                    else:
                        check_counts[check_type]["failed"] += 1
        
        # Create stacked bar chart
        checks = list(check_counts.keys())
        passed = [check_counts[c]["passed"] for c in checks]
        failed = [check_counts[c]["failed"] for c in checks]
        
        x = np.arange(len(checks))
        width = 0.6
        
        p1 = ax.bar(x, passed, width, label='Passed', color='green', alpha=0.8)
        p2 = ax.bar(x, failed, width, bottom=passed, label='Failed', color='red', alpha=0.8)
        
        ax.set_ylabel('Number of Models')
        ax.set_title('CIF Verification Results by Check Type')
        ax.set_xticks(x)
        ax.set_xticklabels(checks, rotation=45, ha='right')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "verification_summary.png"), dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot 2: Final CIF values distribution
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # DeepSurv final values
        deepsurv_finals = []
        deephit_event1_finals = []
        deephit_event2_finals = []
        
        for model_name, stats in self.results["statistics"].items():
            if "deepsurv" in model_name.lower():
                if "mean_at_5years" in stats:
                    deepsurv_finals.append(stats["mean_at_5years"])
            else:  # deephit
                if "event1_mean_at_5years" in stats:
                    deephit_event1_finals.append(stats["event1_mean_at_5years"])
                if "event2_mean_at_5years" in stats:
                    deephit_event2_finals.append(stats["event2_mean_at_5years"])
        
        # DeepSurv distribution
        if deepsurv_finals:
            ax1.hist(deepsurv_finals, bins=20, alpha=0.7, color='blue', edgecolor='black')
            ax1.axvline(np.mean(deepsurv_finals), color='red', linestyle='--', 
                       label=f'Mean: {np.mean(deepsurv_finals):.3f}')
            ax1.set_xlabel('Final CIF Value (5 years)')
            ax1.set_ylabel('Count')
            ax1.set_title('DeepSurv Models - Final CIF Distribution')
            ax1.legend()
        
        # DeepHit distribution
        if deephit_event1_finals and deephit_event2_finals:
            ax2.hist(deephit_event1_finals, bins=15, alpha=0.5, color='green', 
                    label='Event 1', edgecolor='black')
            ax2.hist(deephit_event2_finals, bins=15, alpha=0.5, color='orange', 
                    label='Event 2', edgecolor='black')
            ax2.set_xlabel('Final CIF Value (5 years)')
            ax2.set_ylabel('Count')
            ax2.set_title('DeepHit Models - Final CIF Distribution')
            ax2.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "final_cif_distributions.png"), dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_report(self, output_path: str = "results/final_deploy/verification/verification_report.json"):
        """Save verification report to JSON."""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Add summary
        total_checks = len(self.results["checks"])
        passed_checks = sum(1 for v in self.results["checks"].values() if v["passed"])
        
        self.results["summary"] = {
            "total_checks": total_checks,
            "passed_checks": passed_checks,
            "failed_checks": total_checks - passed_checks,
            "pass_rate": float(passed_checks / total_checks) if total_checks > 0 else 0.0,
            "n_warnings": len(self.results["warnings"]),
            "n_errors": len(self.results["errors"]),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2, cls=NumpyEncoder)
        
        print(f"\nReport saved to: {output_path}")
    
    def run_verification(self, sample_models: int = None):
        """Run verification on all models or a sample."""
        summary = self.load_summary()
        models = summary["results"]
        
        if sample_models:
            # Sample models for testing
            import random
            models = random.sample(models, min(sample_models, len(models)))
        
        print(f"Verifying {len(models)} models...")
        print("=" * 60)
        
        for model_info in models:
            # Verify temporal predictions
            self.verify_model(model_info, dataset="temporal")
            
            # Verify spatial predictions
            self.verify_model(model_info, dataset="spatial")
        
        # Create visualizations
        print("\nCreating visualizations...")
        self.create_visualizations()
        
        # Save report
        self.save_report()
        
        # Print summary
        print("\n" + "=" * 60)
        print("VERIFICATION SUMMARY")
        print("=" * 60)
        print(f"Total checks performed: {self.results['summary']['total_checks']}")
        print(f"Passed: {self.results['summary']['passed_checks']}")
        print(f"Failed: {self.results['summary']['failed_checks']}")
        print(f"Pass rate: {self.results['summary']['pass_rate']:.1%}")
        print(f"Warnings: {self.results['summary']['n_warnings']}")
        print(f"Errors: {self.results['summary']['n_errors']}")
        
        if self.results["errors"]:
            print("\nERRORS:")
            for error in self.results["errors"][:5]:  # Show first 5 errors
                print(f"  - {error}")
            if len(self.results["errors"]) > 5:
                print(f"  ... and {len(self.results['errors']) - 5} more")
        
        if self.results["warnings"]:
            print("\nWARNINGS:")
            for warning in self.results["warnings"][:5]:  # Show first 5 warnings
                print(f"  - {warning}")
            if len(self.results["warnings"]) > 5:
                print(f"  ... and {len(self.results['warnings']) - 5} more")
        
        return self.results["summary"]["pass_rate"] == 1.0


def main():
    """Main function to run CIF verification."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Verify CIF array sanity")
    parser.add_argument("--sample", type=int, default=None, 
                       help="Number of models to sample for testing (default: all)")
    parser.add_argument("--predictions-dir", type=str, 
                       default="results/final_deploy/individual_predictions",
                       help="Directory containing predictions")
    
    args = parser.parse_args()
    
    # Create verifier
    verifier = CIFVerifier(predictions_dir=args.predictions_dir)
    
    # Run verification
    all_passed = verifier.run_verification(sample_models=args.sample)
    
    # Exit with appropriate code
    exit(0 if all_passed else 1)


if __name__ == "__main__":
    main()